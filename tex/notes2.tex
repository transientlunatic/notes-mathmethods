\section{Fields of Scalars}
\label{sec:scalarfields}

A field is an algebraic structure with notions of addition,
subtraction, multiplication, and division, satisfying certain
axioms. The principle examples are $\mathbb{Q}$, $\mathbb{R}$, and
$\mathbb{C}$.

 \begin{definition}[Field of Scalars]
   A field of scalars consists of a set, $F$, whose elements are
   called scalars, together with two algebraic
   operations---addition and multiplication---for combining every
   pair of scalars, $x, y \in F$ to form the new scalars $(x+y) \in
   F$ and $(x \times y) \in F$.  These operations must satisfy the
  field axioms:
  \begin{enumerate}
  \item {\em Associativity}: For $x,y,z \in F$,
    \begin{align*}
      (x+y)+ z &= x+(y+z) \\
      (x\times y)\times z &= x \times (y \times z)
    \end{align*}
  \item {\em Distributivity}:
    \begin{align*}
      (x+y) \times z &= x \times z + y \times z \\
      z \times (x + y) &= z \times x + z \times y
    \end{align*}
  \item {\em Commutativity}:
    \begin{align*}
      x + y &= y + x \\
      x \times y &= y \times x
    \end{align*}
  \item {\em Zero and Unity}: There are unique and distinct
    elements, $0, 1 \in F$, such that
    \begin{align*}
      x+0 &= x = 0 + x \\
      x \times 1 &= x = 1 \times x
    \end{align*}
  \item {\em Additive and multiplicative inverses}: For $x \in F$
    there exists a unique element $-x \in F$, for which
    \[ x + (-x) = 0 = (-x)+x \] For each non-zero $y \in F$ there
    is a unique element $y^{-1} \in F$ for which \[ y \times
    (y^{-1}) = 1 = (y^{-1})\times y \]
  \end{enumerate}
 \end{definition}

\section{Vector Spaces}
\label{sec:vectorspace}

\begin{definition}[Vector Space]
  A vector space over $F$ is a set, $\vs{V}$ of elements called vectors on which addition, $\vec{u}+\vec{v}$ of vectors $\vec{u}$ and $\vec{v}$, is defined, scalar multiplication, $\lambda \vec{u}$ of a vector $\vec{u}$ by a scalar $\lambda$ from $F$ is defined and the following axioms hold:\\
\begin{subequations}
  \begin{align}
     \vec{u}+\vec{v} &\in \vs{V} \\
     (\vec{u} + \vec{v}) + \vec{w} &= \vec{u}+(\vec{v}+\vec{w}) \\
     \vec{u}+\vec{v} &= \vec{v}+\vec{u} \\
     \exists \vec{0} \in \vs{V} & \mid \vec{u}+\vec{0} = \vec{u}=\vec{0}+\vec{u} \\
     \forall \vec{u} \in \vs{V} \exists - \vec{u}\in \vs{V} & \mid \vec{u}+(-\vec{u}) = \vec{0} = (-\vec{u})+\vec{u} \\
     \lambda \vec{u} \in \vs{V} \\
     \forall \vec{u}, \vec{v} \in \vs{V} , \forall \lambda, \mu \in F, & \lambda(\vec{u}+\vec{v}) = \lambda \vec{u}+\lambda \vec{v}
  \end{align}
\end{subequations}
\end{definition}
\begin{definition}[Vector Subspace]
  A non-empty subset, $w$, of a vector space $\mathsf{V}$ over $F$, such that
  \begin{subequations}
    \begin{equation}
      \label{eq:subspaceaddclose}
      \vec{w}_1 + \vec{w}_2 \in w \qquad \forall \vec{w}_{1,2} \in w
    \end{equation}
    \begin{equation}
      \label{eq:scalarmultsubsclose}
      \lambda \vec{w} \in w \qquad \forall \vec{w} \in w, \forall \lambda \in F
    \end{equation}
  \end{subequations}
\end{definition}
\begin{definition}[Vector space sum]
  Let $\vs{U}_1$ and $\vs{U}_2$ be subspaces of a vector space $\vs{V}$, then the sum, $\vs{U}_1 + \vs{U}_2$ is defined,
  \begin{equation}
    \label{eq:vectorspacesum}
    \vs{U}_1 + \vs{U}_2 = \left\{ \vec{u}_1 + \vec{u}_2 \in \vs{V} \mid \vec{u}_1 \in \vs{U}_1 \wedge \vec{u}_2 \in \vs{U}_2 \right\}
  \end{equation}
i.e.\ $\vs{U}_1 + \vs{U}_2$ is the set of vectors in $\vs{V}$, that, expressed as a vector in $\vs{U}_1$ added to a vector in $\vs{U}_2$.
\end{definition}
\begin{definition}[Vector Direct Sum]
  A sum $\vs{U}_1 + \vs{U}_2$ in which every element can be expressed uniquely in the form $\vec{u}_1 + \vec{u}_2$, with $\vec{u}_1 \in \vs{U}_1$, and $\vec{u}_2 \in \vs{U}_2$ is called a direct sum, and is denoted
\[ \vs{U}_1 \oplus \vs{U}_2 \]
\end{definition}
\begin{lemma}
  The sum $\vec{u}_1 + \vec{u}_2$ is the direct sum, $\vec{u}_1 \oplus \vec{u}_2$ iff $\vec{u}_1 \cap \vec{u}_2 = \{\vec{0}\}$
\end{lemma}
\begin{definition}[Linear Combination]
  Let $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n$ be vectors in the vector space $\vs{V}$ over the field $F$. A linear combination of these vectors is a vector of the form
\[ \lambda_1 \vec{u}_1 + \lambda_2 \vec{u}_2 + \cdots + \lambda_n \vec{u}_n \]
with $\lambda_1, \lambda_2, \dots, \lambda_n \in F$.
\end{definition}
\begin{definition}[Span of a vector space]
   Let $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n$ be vectors in the vector space $\vs{V}$ over the field $F$,
then the subspace of $\vs{V}$ spanned by these vectors is denoted 
\[ {\rm sp}(\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n) \]
and is defined by
\[ \left\{ \lambda_1 \vec{u}_1 + \lambda_2 \vec{u}_2 + \cdots +
  \lambda_n \vec{u}_n \mid \lambda_1, \lambda_2, \dots, \lambda_n \in
  F \right\} \] So the supspace spanned by this sequence of vectors is
the set of all linear combinations which may be formed from the
sequence.
\end{definition}
\begin{definition}[Finite Dimensional Vector Space]
  A finite dimensional vector space is one which is spanned by a
  finite sequence of vectors.
\end{definition}
\begin{definition}[Linearly Independent Sequence]
  A sequence of vectors $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n \in
  \vs{V}$ is called a linearly independent sequence iff
  \[ \lambda_1 \vec{u}_1 + \lambda_2 \vec{u}_2 + dots + \lambda_n
  \vec{u}_n = \vec{0}\] is only possible when
  \[ \lambda_1 + \lambda_2 + \cdots +\lambda_n = 0 \] with $\lambda_1,
  \lambda_2, \dots , \lambda_n \in F$.
\end{definition}
\begin{theorem}
  If $\vs{W}$ is a subspace of $\vs{V}$ such that it is spanned by
  $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n$, then there is a subspace
  of this sequence which is linearly independent and still spans
  $\vs{W}$.
\end{theorem}
\begin{definition}[Basis]
  A basis is a linearly independent sequence of vectors which is a
  span of a vector space.
\end{definition}
\begin{lemma}[Expressing elements in a vector space]
  Suppose $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n$ is a basis of
  \vs{V}. Then every element can be uniquely expressed as a linear
  combination of the sequence. The unique scalar multiples of each are the \emph{components} of the element $\vec{x} \in \vs{V}$.
\end{lemma}
\begin{theorem}
  Suppose $\vs{V}$ has a basis $\vec{u}_1, \vec{u}_2 \dots
  \vec{u}_n$. Then any sequence of vectors $\vec{w}_1, \vec{w}_2,
  \dots \vec{w}_m \in \vs{V}$ with $m > n$ is linearly dependent.
\end{theorem}
\begin{definition}[Dimension of a vector space]
  Suppose $\vs{V}$ is finite dimensional. Then the dimension of $\vs{V}$, denoted $\dim(\vs{V})$, is the number of vectors in any basis of $\vs{V}$.
\end{definition}
\begin{lemma}[Conditions for a basis]
  A sequence of vectors in $\vs{V}$ is a basis provided it possesses any two of the following conditions,
  \begin{enumerate}
  \item the sequence spans $\vs{V}$
  \item the sequence is linearly independent
  \item $n = \dim(\vs{V})$
  \end{enumerate}
\end{lemma}
\begin{lemma}[Properties of a vector subspace]
  Suppose $\vs{V}$ is finite dimensional; let $\vs{W}$ be a subspace of $\vs{V}$, then,
  \begin{enumerate}
  \item $\vs{W}$ is finite dimensional
  \item $\dim(\vs{W}) \leq \dim(\vs{V})$
  \item If $\vs{W} \neq \vs{V}$ then $\dim(\vs{W}) < \dim(\vs{V})$
  \item Any basis of $\vs{W}$ can be extended to be a basis of
    $\vs{V}$.
  \end{enumerate}
\end{lemma}

\subsection{Inner Product Spaces}
\label{sec:innerproduct}
In this section, let $\vs{V}$ denote a vector space over the field
$\mathbb{R}$, let $\vec{u}, \vec{v},$ and $\vec{w}$ be vectors from
$\vs{V}$, and $a, b, c,$ and $d$ be scalars from the field
$\mathbb{R}$.
\begin{definition}[Real Inner Product]
  Suppose to each pair of vectors $\vec{u},\vec{v} \in \vs{V}$ there
  is assigned a real number, denoted $\langle \vec{u}, \vec{v}
  \rangle$, which is called the real inner product on $\vs{V}$ if it
  satisfies the axioms
  \begin{enumerate}
  \item (Linearity)
    \[ \langle a \vec{u}_1 + b \vec{u}_2 , \vec{v} \rangle = a \langle
    \vec{u}_1, v \rangle + b \langle \vec{u}_2, \vec{v} \rangle \]
  \item (Symmetry)
    \[ \langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u}
    \rangle \]
  \item (positive definite)
    \[ \langle \vec{u}, \vec{u} \rangle \geq 0 \text{ and }
       \langle \vec{u}, \vec{u} \rangle = 0 \text{ iff } \vec{u} = 0. 
    \]
  \end{enumerate}
\end{definition}
\begin{definition}[Real Inner Product Space]
  A vector space, $\vs{V}$ on which a real inner product is defined is
  called a real inner product space.
\end{definition}
\begin{definition}[Norm of a vector]
  The third axiom of the inner product requires that it always be
  positive. This allows the definition of the norm,
  \[ ||\vec{u}|| = \sqrt{ \langle \vec{u}, \vec{u} \rangle } \] which
  is a measure of the length of the vector.
\end{definition}
There are numerous examples of inner product spaces, from Euclidean $n$-spaces (perhaps the most day-to-day example), function and polynomial space, matrix space, and Hilbert space.
\begin{theorem}[Cauchy-Schwarz Inequality]
  For any vectors $\vec{u}$ and $\vec{v}$ in an inner product space $\vs{V}$, 
  \[ \langle \vec{u}, \vec{v} \rangle^2 \leq \langle \vec{u}, \vec{u}
  \rangle \langle \vec{v}, \vec{v} \rangle \quad \text{or} \quad |
  \langle \vec{u}, \vec{v} \rangle \leq ||\vec{u}|| ||\vec{v}|| \]
\end{theorem}
\begin{theorem}[Properties of the Norm]
  Let $\vs{V}$ be an inner product space. Then the norm in $\vs{V}$ satisfies the following properties
  \begin{enumerate}
  \item $||\vec{v}|| \geq 0$ and $||\vec{v}||=0$ if and only if
    $\vec{v}=0$.
  \item $||k \vec{v}|| = |k| ||\vec{v}||$
  \item $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
  \end{enumerate}
\end{theorem}
\begin{definition}[Orthogonal Vectors]
  Two vectors, $\vec{u}, \vec{v} \in \vs{V}$ are orthogonal iff
  \[ \langle \vec{u}, \vec{v} \rangle = 0 \]
\end{definition}
\begin{definition}[Orthonormal Vectors]
  Two vectors, $\vec{u}, \vec{v} \in \vs{V}$ are orthonormal iff
  \[ \langle \vec{u}_i, \vec{u}_j \rangle =
  \begin{cases}
    0 & \vec{u} \neq \vec{v} \\
    1 & \vec{u} = \vec{v}
  \end{cases}
  \]
\end{definition}
It follows that a sequence of orthogonal or orthonormal vectors is
linearly independent, and so can form a basis. In order to
orthogonalise a basis use the Gram-Schmidt Process.  Let $\vec{u}_1,
\vec{u}_2$ be linearly independent vectors with an angle $\theta$
between them. Then
\[ \theta = \frac{\langle \vec{u}_1, \vec{u}_2 \rangle}{||\vec{u}_1||
  || \vec{u}_2||} \] so $\vec{u}_2$ can be expressed as the sum of two
vectors in the direction of $\vec{v}_2$ (a new, vector orthogonal to
$\vec{u}_1$, and $\vec{u}_1$. For convenience, let $\vec{v}_1 = \vec{u}_1$.

\begin{algorithm}[Gram-Schmidt Process]
  For a sequence of non-orthonormal, linearly independent vectors
  $\vec{u}_1, \vec{u}_2, \dots, \vec{u}_n$, in order to produce an
  orthonormal sequence, $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n$,
  \begin{enumerate}
  \item Let $\vec{v}_1$
  \item Set 
\[\vec{v}_k = \vec{u}_k - \sum_{i=1}^{k-1} \frac{\langle \vec{v}_i, \vec{u}_k \rangle}{\langle \vec{v}_i, \vec{u}_i \rangle} \cdot \vec{v}_i \]
$\forall k \in \{ 1, ..., n \}$.
\item Normalise each vector by \[ \hat{v}_i = \frac{\vec{v}_i}{||\vec{v}_i||} \]
  \end{enumerate}
\end{algorithm}
\begin{definition}[Complex inner product spaces]
  Let $\vs{V}$ be a vector space over $\mathbb{C}$. Suppose each pair of vectors 
\end{definition}
\section{Matrices}
\label{sec:matrixtheory}

\begin{definition}[Matrix]
  A matrix, $A$ over a field $K$ is a rectangular, $n \times m$, array of scalars, which is usually represented in the form
\[ A =
\begin{pmatrix}
  a_{11} & a_{12} & \cdots & a_{1n} \\
  a_{21} & a_{22} & \cdots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\]
Both the operations of addition and multiplication are defined for matrices.
\end{definition}
\begin{definition}[Main Diagonal]
  The main diagonal of a matrix are the entries $a_{ij}$ where $j=i$.
\end{definition}
\begin{definition}[Square Matrix]
  A matrix is called a square matrix if it is of shape $(n \times n)$.
\end{definition}

\begin{definition}[Upper and Lower Triangular Matrices]
  If a square matrix has only zeros below every entry in the main
  diagonal it is an upper triangular matrix. If the matrix has only
  zeros above every entry in the main diagonal it is lower triangular.
\end{definition}

\begin{definition}[Diagonal Matrix]
  A matrix is called diagonal if all of the entries lying off the main
  diagonal are zero. A diagonal matrix may also be written as 
  \[ \operatorname{diag}(a_{11}, a_{22}, \cdots, a_{nn}) \]
\end{definition}
It should be noted that a diagonal matrix is both upper and lower
triangular.

\begin{definition}[Cofactors of a  Matrix]
  The cofactors, $A^{ij}$ of an $n \times n$ matrix $A$ is the
  $(n-1) \times (n-1)$ matrix containing the elements of $A$ excluding
  those in the $i^{\rm th}$ row and the $j^{\rm th}$ column.
\end{definition}

\begin{definition}[Cofactor Matrix]
  A cofactor matrix is a matrix whose elements are all cofactors, and has the form
\[ C = 
\begin{pmatrix}
  C_{11} & C_{12} & \cdots & C_{1n} \\
  C_{21} & C_{22} & \cdots & C_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  C_{n1} & C_{n2} & \cdots & C_{nn}
\end{pmatrix}
\]
\end{definition}

\subsubsection{Matrix Operations}
\label{sec:matops}

\begin{definition}[Matrix Transpose]
  The transpose of a square matrix $A$ is defined
  \[ \qty[\trans{A}]_{ij} = \qty[A]_{ji} \] that is, the transpose of
  a matrix is the original matrix reflected about its main diagonal.
\end{definition}

\begin{definition}[Matrix Trace]
  The trace of a square matrix, $A$ is the sum of the elements in its main diagonal, ie.\
  \[ \trace(A) = \sum^n_{i=1} a_{ii}\] where $a_{ij}$ are the elements
  of $A$.
\end{definition}

\begin{definition}[Complex Conjugate]
  The complex conjugate, $\conj(A)$ of a matrix $A$ over $\mathbb{C}$
  is the matrix in which every element is the complex conjugate of the
  corresponding element of $A$.
  \[ \qty[\conj(A)]_{ij} = \qty( \qty[A]_{ij} )^{*} \]
\end{definition}

\begin{definition}[Hermitian Conjugate]
  The hermitian conjugate, $A^{\dagger}$ of a matrix $A$ is the matrix
  \[ A^{\dagger} = \conj(\trans{A}) = \trans{\conj(A)} \]
\end{definition}

\begin{definition}[Matrix Determinant]
  Let $A$ be a square matrix, and $a_{i,j}$ be the elements of $A$, then, the
  determinant of the matrix, denoted $\det(A)$ or $|A|$, is defined
  \[ \det(A) = \sum_{i_1, i_2, \dots, i_n = 1}^n \epsilon_{i_1 \cdots
    i_n} a_{1,i_1} \cdots a_{n,i_n} \]
\end{definition}

\begin{theorem}[Properties of the Determinant]
  For $n \times n$ matrices, $A,B$, a triangular matrix, $D$, and
  scalar $c$,
  \begin{enumerate}
  \item $\det(I_n) = 1$
  \item $\det(A^{\rm T}) = \det(A)$
  \item $\det(A^{-1}) = \det^{-1}(A)$
  \item $\det(AB) = \det(A) \det(B)$
  \item $\det{cA} = c^n \det{A}$
  \item $\det{D} = \prod_{i=1}^n d_{i,i}$ (for $d_{i,j} \in D$)
  \end{enumerate}
\end{theorem}

\begin{lemma}[Determinant of a triangular matrix]
  The determinant of any triangular matrix is the product of the
  entries on its main diagonal.
\end{lemma}

\begin{lemma}
  Let $E(\Theta)$ be an elementary matrix for an elementary row
  operation $\Theta$, and $B = E(\Theta)A$ for matrices $A$ and $B$. Then,
  \[ \det(B) = 
  \begin{cases}
    - \det(A) & \text{ if } \Theta \text{ is } R_i \leftrightarrow R_j \\
    \lambda \det(A) & \text{ if } \Theta \text { is } R_i \to R_j \\
    + \det(A) & \text{ if } \Theta \text{ is } R_i \to R_i + \lambda R_j
  \end{cases}
\]
\end{lemma}

\begin{definition}[Invertible Matrix]
  An $n \times n$ matrix $A$ is called invertible if there exists an
  $n \times n$ matrix $B$ such that
  \[ AB = BA = I \]
  with $I$ the identity matrix.
\end{definition}
There are a number of techniques for identifying the inverse of a
matrix, one of which is Cramer's rule, and the other is Eigenvalue
decomposition.

\begin{definition}[Adjugate Matrix]
  The adjugate matrix is the transpose of the cofactor matrix.
\end{definition}

\begin{algorithm}[Cramer's Rule]
  Cramer's rule is a process for inverting a square matrix.  Let $A$
  be a square matrix, with $\det(A) \neq 0$. Then 
  \[A^{-1} = \frac{1}{|A|} \trans{C} \] where $\trans{C}$ is the
  adjugate matrix of $A$. This process is highly inefficient for large
  matrices.
\end{algorithm}

\subsubsection{Special Matrices}
\label{sec:special}

\begin{definition}[Identity Matrix]
  The matrix $I$ is called the identity matrix and has the form 
  \[ \operatorname{diag}(1,1,\dots, 1) \]
\end{definition}
\begin{definition}[Symmetric Matrix]
  A matrix $A$ over $\mathbb{R}$ is a symmetric matrix if $A^{\rm T} = A$.
\end{definition}
\begin{lemma}[Eigenvectors and Eigenvectors of Symmetric Matrices]
  Let $A$ be a real, $n \times n$ symmetric matrix. The eigenvalues
  are all real, and each has a corresponding real unit
  eigenvector. Further, any eigenvectors corresponding to distinct
  eigenvalues are orthogonal.
\end{lemma}
\begin{definition}[Hermitian Matrix]
  A matrix $A$ over $\mathbb{C}$ is a hermitian matrix if $A^\dagger = A$.
\end{definition}
\begin{definition}[Orthogonal Matrix]
  A matrix $Q$ over $\mathbb{R}$ is called \emph{Orthogonal} if
  $Q^{\rm T} = Q^{-1}$.
\end{definition}
\begin{lemma}[Inverse of an orthogonal matrix]
  An orthogonal matrix $P$ is invertible, and $P^{-1} = \trans{P}$.
\end{lemma}
\begin{definition}[Unitary Matrix]
  A matrix $U$ over $\mathbb{C}$ is called \emph{Unitary} if the
  Hermitian conjugate of $U$, $U^{\dagger} = U^{-1}$.
\end{definition}
\begin{definition}[Markov Matrix]
  A markov matrix is a square matrix in which every element is
  non-negative, and in ehich all of the entries in each column sum to
  1.
\end{definition}
Every Markov matrix defines a difference equation \[ X_{n+1} = M
X_n \] for Markov matrix $M$, and non-negative columns $X_n$.
\subsubsection{Eigenquantities}
\label{sec:eigen}

\begin{definition}[Eigenvectors \& Eigenvalues]
  Let $A$ be an $n \times n$ matrix with entries from $F$. A non-zero
  vector $\vec{x} \in F^n$, such that \[A \vec{x} = \lambda \vec{x} \]
  for some scalar $\lambda \in F$. Then $\vec{x}$ is an
  \emph{eigenvector} of $A$, and $\lambda$ is the corresponding
  eigenvalue.
\end{definition}

\begin{lemma}
  Let $A$ be a square matrix over $\mathbb{R}$, with an eigenvalue,
  $\lambda$ in $\mathbb{R}$. Then $A$ has a real eigenvector which
  corresponds to $\lambda$.
\end{lemma}


\begin{definition}[Characteristic Polynomial]
  Let $A$ be an $n \times n$ matrix. The characteristic polynomial
  $\chi_A (t)$ of $A$ is defined
  \[ \chi_A (t) = \det (t I - A) \] with $I$ the identity matrix.
\end{definition}

\begin{lemma}
  For an $n \times n$ matrix $A$ the polynomial $\chi_A (t)$ is of
  degree $n$ and is monic (i.e.\ the coefficient of $t^n$ is
  1). Suppose that $\chi_A (t) = t^n + c_{n-1} t^{n-1} + \cdots + c_1
  t + c_0$, then
  \[ c_{n-1} = - \tr(A) \qquad c_0 = (-1)^n \det(A) \]
\end{lemma}

\begin{lemma}
  Let $A, B$ be $n \times n$ matrices, with $B$ being invertible, then,
  \[\chi_{BAB^{-1}}(t)  = \chi_A (t)\]
\end{lemma}

\begin{definition}[Matrix Polynomial]
  Consider a polynomial
  \[ p(t) = a_k t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0 \] with
  coefficients drawn from a field $F$. The $n \times n$ matrix $A$ is
  said to satisfy the polynomial $p(t)$ if
  \[ p(A) = a_k A^k + a_{k-1} A^{k-1} + \cdots + a_1 A + a_0 I = 0 \]
  with the right hand side being the zero matrix.
\end{definition}

\begin{theorem}[Cayley-Hamilton Theorem]
  Let $A$ be an $n \times n$ matrix, then \[\chi_A(A) = 0 \]
\end{theorem}

\begin{corollary}
  For an $n \times n$ matrix $A$, if $\det(A) \neq 0$ then $A$ is
  invertible.
\end{corollary}

\begin{theorem}
  Let $A$ be a complex square matrix. Since $\chi_A(t)$ has degree
  $n$, $A$ has $n$ complex eigenvalues (which may have multiplicity).
\end{theorem}

\begin{lemma}
  Let $\lambda_1, \lambda_2, \dots $l$_n$ be the eigenvalues of an $n
  \times n$ matrix over $\mathbb{C}$. Then
  \[ \sum_{i=1}^n \lambda_i = \trace(A) \] and \[ \prod_{i=1}^n
  \lambda_i = \det(A) \]
\end{lemma}

\begin{theorem}[Eigenvalues of a Hermitian Matrix]
  \label{the:eigenvaluehermitian}
  The eigenvalues of a Hermitian matrix are real.

  \begin{proof}
    Let $A$ be a Hermitian matrix; by definition $A = A^\dagger$.  Let
    $\lambda$ be an eigenvector of $A$.  Let $\vec{v}$ be an
    eigenvector corresponding to the eigenvalue $\lambda$.  Let
    $\braket{\cdot}$ be the inner product on $\mathbb{C}$, using
    braket notation, so,
    \begin{align*}
      \lambda \braket{\vec{v}} &= \braket{\lambda \vec{v}}{\vec{v}} \\
      &= \braket{A \vec{v}}{\vec{v}} \\
      &= \braket{\vec{v}}{A^{\dagger}\vec{v}} \\
      &= \braket{\vec{v}}{A\vec{v}}\\
      &= \braket{\vec{v}}{\lambda \vec{v}} \\
      &= \lambda^{*} \braket{\vec{v}}
    \end{align*}
    Since $\braket{v} \neq 0$ it follows that $\lambda = \lambda^{*}$,
    and so must be real.
  \end{proof}
\end{theorem}

\subsubsection{Roots of Polynomials}
\label{sec:polynomialroots}

Any polynomial $p(t) \in \mathbb{C}[t]$ of positive degree $n$ can be
factored into $n$ linear factors
\[ p(t) = c(t-\alpha_1)(t-\alpha_2)\cdots (t-\alpha_n) \] where $c
\neq 0$, and the roots $\alpha_1, \alpha_2, \dots \alpha_n$ are
uniquely determined from their order. Then gathering repeated factors,
\[ p(t) = c(t-\beta_1)^{r_1} (t-\beta_2)^{r_2} \cdots
(t-\beta_m)^{r_m} \] where $\beta_1, \beta_2, \dots \beta_m$ are the
distinct complex roots of $p(t)$, and each $r_k \ge 1$ which is the
algebraic multiplicity of $\beta_k$.

\begin{lemma}
  Let
  \[ p(t) = t^n + a_{n-1}t^{n-1} + \cdots + a_1 t + a_0 \] be a monic
  polynomial which can be factored \[ p(t) =
  (t-\alpha_1)(t-\alpha_2)\cdots (t-\alpha_n) \] where $\alpha_1,
  \dots \alpha_n$ are complex roots, then
  \[ \sum_{i=1}^n \alpha_i = - a_{n-1} \qquad \prod_{i=1}^n = (-1)^n
  a_0 \]
\end{lemma}

\subsubsection{Diagonalisation}
\label{sec:diagonalisation}

\begin{definition}[Similarity]
  Let $A,B$ be square matrices with entries from $F$. Then $A$ is
  similar to $B$ if \[ B = P^{-1} A P \] for some invertible square
  matrix $P$ with entries from $F$.
\end{definition}

\begin{lemma}[Properties of similar matrices]
  \begin{enumerate}
  \item Any square matrix $A$ is similar to itself.
  \item If $A$ is similar to $B$, then $B$ is similar to $A$.
  \item If $A$ is similar to $B$, and $B$ is similar to $C$, then $A$
    is similar to $C$.
  \end{enumerate}
\end{lemma}

\begin{lemma}[Characteristic Polynomial of similar matrices]
  Let $A, B$ be similar square matrices. Then $A$ and $B$ have the
  same characteristic polynomial, and hence the same trace,
  determinant, and eigenvalues.
\end{lemma}

\begin{definition}[Diagonalisability]
  Let $A$ be a square matrix over $F$. If $A$ is similar to a diagonal
  matrix over $F$ then A is diagonalisable over $F$.
\end{definition}

\begin{theorem}
  Let $A$ be a square matrix over $F$. $A$ is diagonalisable over $F$
  iff there exists an invertible matrix $P$ over $F$ whose columns are
  the eigenvectors of $A$. That is
  \[ P^{-1} A P = D \] for $D$ a diagonal matrix, $D =
  \diag(\lambda_1, \dots, \lambda_n)$ for distinct eigenvalues
  $\lambda_1, \dots, \lambda_n$.
\end{theorem}

\begin{definition}[Jordan-normal Form]
  A matrix is said to be in Jordan-normal form if all of the non-zero
  entries off the main diagonal are immediately above an element on
  the main diagonal, and have identical diagonal elements to the left
  and below them.
\end{definition}

\begin{corollary}
  Not all matrices are diagonalisable, but over $\mathbb{C}$ it is
  always possible to find a matrix $P$ such that \[P^{-1} A P = J \]
  for a matrix $J$ which is in Jordan-normal form.
\end{corollary}

\section{Linear Mappings}
\label{sec:linearmapping}
Let $X,Y,Z$ denote sets.
\begin{definition}[Mapping]
  A mapping $f:X \to Y$ is a rule associating every element in $X$
  with a unique member of $Y$. $X$ is the domain, $Y$ is the codomain.
\end{definition}

\begin{definition}[Function Composition]
  Given two mappings $f:X \to Y$ and $g:Y \to Z$, the composition, $g
  \circ f: X \to Z$ is the mapping
  \[ (g \circ f)(x) = g(f(x)) \qquad \forall x \in X \]
\end{definition}

\begin{definition}[Identity Mapping]
  The identity mapping \[ \idmap_X : X \to X \] is defined by \[
  \idmap_X(x) = x \quad \forall x \in X \]
\end{definition}

\begin{definition}[Zero Mapping]
  Provided $Y$ contains a zero element $0$, the zero mapping, $0_x:X
  \to Y$ is defined
  \[ 0_x(x) = 0 \quad \forall x \in X \]
\end{definition}

\begin{definition}[Injective, surjective, and bijective mappings]
  A mapping $f:X \to Y$ is injective if for all $x_1, x_2 \in X$,
  \[ f(x_1) = f(x_2) \implies x_1 = x_2 \] A mapping $f: X \to Y$ is
  called surjective if, for every $y \in Y$ there exists at least one
  $x \in X$ for which $y = f(x)$.\\
  A mapping is bijective if it is both injective and surjective.
\end{definition}

\begin{lemma}
  A mapping $f:X \to Y$ is bijective iff there is an inverse
  mapping \[ h: Y \to X \] such that \[ h \circ f = \idmap_X \quad
  \text{and} \quad f \circ h = \idmap_Y \]
\end{lemma}

\begin{definition}[Linear Mapping]
  Let $\vs{V}, \vs{W}$ be vector spaces over a field $F$.
A linear mapping (or linear transformation) from $\vs{V}$ to $\vs{W}$ is a mapping \[ f: \vs{V} \to \vs{W} \] such that 
\[ f(\alpha \vec{u} + \beta \vec{v}) = \alpha f(\vec{u}) + \beta f(\vec{v}) \] $\forall \alpha, \beta \in F$ and $\vec{u}, \vec{v} \in \vs{V}$.
\end{definition}
Examples of linear mappings are linear functions, and the linear differential operator.

\begin{definition}[Matrix of a linear mapping with respect to a given basis]
  Let $f:\vs{V} \to \vs{W}$ be a mapping between two vector spaces
  $\vs{V}$ and $\vs{W}$ over $F$. Suppose $S_1: \vec{v}_1, \vec{v}_2,
  \dots, \vec{v}_n$ is a basis for $\vs{V}$, and $S_2: \vec{w}_1,
  \vec{w}_2, \cdots \vec{w}_m$ is a basis for $\vs{W}$.  Then, for $j
  = 1,2, \dots, n$,
  \[ f(\vec{v}_j) = a_{1j} \vec{w}_1 + a_{2j} \vec{w}_2 + \cdots +
  a_{mj} \vec{w}_m \] for scalars $a_{ij}$, with $i = 1,2, \dots, m$,
  $j=1,2, \dots, n$, then the matrix \[ A = \qty[a_{ij}] \] is the
  matrix of $f$ with respect to the bases $S_1$ and $S_2$.
\end{definition}
\begin{lemma}
  Let $f:\vs{V} \to \vs{W}$ and $g:\vs{V}\to \vs{W}$ be linear
  mappings of vector spaces over $F$, and let $\gamma , \delta \in
  F$. Then the mapping $\gamma f + \delta g : \vs{V} \to \vs{W}$
  defined by the rule
  \[ (\gamma f + \delta g)(\vec{v}) = \gamma f(\vec{v}) + \delta
  g(\vec{v}) \qquad (\vec{v} \in \vs{V}) \] is a linear mapping.
\end{lemma}
\begin{definition}[Kernel of a linear mapping]
  Let $f:\vs{V} \to \vs{W}$ be a linear mapping between the vecotr
  spaces $\vs{V}$ and $\vs{W}$ over $F$. The kernel of $f$ is the set
  of vectors from the domain of $f$ which are mapped to the zero
  vector of the codomain $\vs{W}$. That is, \[ \ker(f) = \{ \vec{v}
  \in \vs{V} : f(\vec{v}) = \vec{0} \} \]
\end{definition}
\begin{definition}[Image of a linear mapping]
  Let $f:\vs{v} \to \vs{W}$ be a linear mapping of vector spaces over
  $F$. The image is \[ \img(f) = \{ f(\vec{v}) : \vec{v} \in \vs{V}
  \} \] the set of vectors in the codomain, $\vs{W}$, which are mapped
  to by at least one vector from $\vs{V}$.
\end{definition}
\begin{lemma}
  Let $f: \vs{V} \to \vs{W}$ be a linear mapping of vector spaces over $F$, then
  \begin{itemize}
  \item $\ker(f)$ is a subspace of $\vs{V}$.
  \item $\img(f)$ is a subspace of $\vs{W}$.
  \end{itemize}
\end{lemma}
\begin{lemma}
  Let $f: \vs{V} \to \vs{W}$ be a linear mapping of vector spaces then,
  \begin{enumerate}
  \item $f$ is injective iff $\ker(f) = \{ \emptyset \}$
  \item $f$ is surjective iff $\img(f) = \vs{W}$
  \item $f$ is bijective iff $\ker(f) = \{ \emptyset \}$ and $\img(f) = \vs{W}$
  \end{enumerate}
\end{lemma}
\begin{definition}[Rank and Nullity]
  Let $f: \vs{V} \to \vs{W}$ be a linear mapping of vector spaces over
  $F$.\\
  The rank of $f$, denoted $\rank(f)$ is defined \[ \rank(f) =
  \dim(\img(f)) \] and the nullity of $f$ is denoted $\nullity(n)$,
  and is defined \[ \nullity(f) = \dim(\ker(f)) \]
\end{definition}
\begin{lemma}
  Let $f: \vs{V} \to \vs{W}$ be a linear mapping between finite
  dimensional vector spaces over $F$, then
  \begin{enumerate}
  \item $f$ is injective iff $\nullity(f)=0$
  \item $f$ is surjective iff $\rank(f) = \dim(\vs{W})$
  \item $f$ is bijective iff $\nullity(f) = 0$ and $\rank(f) =
    \dim(\vs{W})$
  \end{enumerate}
\end{lemma}
\begin{theorem}[Rank-nullity Theorem]
  Let $f: \vs{V} \to \vs{W}$ be a linear mapping between vector spaces
  over $F$, where $\vs{V}$ is finite-dimensional, then 
  \[ \dim(\vs{V}) = \rank(f) + \nullity(f) \]
\end{theorem}
\begin{lemma}
  Let $ \vec{y} = A \vec{x}$ be a representation of a vector with
  respect to a basis $\vec{v}^{\prime}$, and $\vec{z} = B \vec{x}$ be
  a represetation of the vector with respect to a basis
  $\vec{w}^{\prime}$ The matrix of a linear mapping $f$ with respect
  to the bases $\vec{v}^{\prime}$ and $\vec{w}^{\prime}$ will be
  $B^{-1} F A$ for $F$ the matrix of $f$.
\end{lemma}
\begin{lemma}
  Suppose $\vs{U}, \vs{V}, \vs{W}$ are finite dimensional vector
  spaces, and that $f:\vs{U} \to \vs{W}$ and $g:\vs{V} \to \vs{W}$ are
  linear mappings. Suppose $\vec{u}_1, \dots, \vec{u}_m$, $\vec{v}_1,
  \dots, \vec{v}_n$, and $\vec{w}_1, \dots, \vec{w}_n$ are bases of
  each vector space. Let $F, G$ respectively represent the matrices of
  $f, g$.\\ Then the composition $g \circ f: \vs{U} \to \vs{W}$ with
  respect to these bases is $GF$.
\end{lemma}
\begin{definition}[Row Rank]
  Let $F$ be a field, and $A \in M_{m \times n}(F)$ be an $m \times n$
  matrix over $F$. The row rank of $A$ is the number of non-zero rows
  in the reduced echelon matrix row equivalent of $A$.
\end{definition}
\begin{definition}[Column Rank]
  Let $F$ be a field, and $A \in M_{m \times n}(F)$ be an $m \times n$
  matrix over $F$. The column rank of $A$ is the number of non-zero
  columns in the reduced echelon matrix column equivalent of $A$.
\end{definition}
\begin{lemma}
  For any $m \times n$ matrix, $A$, the row rank and the column rank are equal.
\end{lemma}
\begin{definition}[Rank of a Matrix]
  The rank of a matrix, $A$, is \[ \rank(A) = \rank(f) =
  \dim(\img(f)) \]
\end{definition}
\begin{lemma}
  Let $A$ be a complex $m \times n$ matrix, and let $\lambda \in
  \mathbb{C}$ be an eigenvalue of $A$. Then
  \[ S = \{ \vec{x} \in \mathbb{C}^n : A \vec{x} = \lambda \vec{x}
  \} \] is a subspace of $\mathbb{C}^n$.
\end{lemma}
\begin{definition}[Eigenspace]
  Let $A$ be an $n \times n$ complex matrix, and let $\lambda \in
  \mathbb{C}$ be an eigenvalue of $A$. Then the $\lambda$-eigenspace
  of $A$ is \[ \Eig_A(\lambda) = \{ \vec{x} \in \mathbb{C}^n : A
  \vec{x} = \lambda \vec{x} \}\]
\end{definition}
\begin{definition}
  Let $A$ be an $n \times n$ complex matrix, and let $\lambda \in
  \mathbb{C}$ be an eigenvalue of $A$. The geometric multiplicity of
  $\lambda$ is $\dim \Eig_A(\lambda)$, and the dimension of the
  $\lambda$-eigenspace of $A$.
\end{definition}
\begin{lemma}
  The geometric multiplicity of an eigenvalue $\lambda$ is less than
  or equal to the algebraic multiplicity of $\lambda$ in the
  characteristic polynomial of $A$.
\end{lemma}
\begin{lemma}
  Let $A$ be an $n \times n$ matrix over $\mathbb{C}$. Suppose
  $\lambda_1, \dots, \lambda_k$ are distinct eigenvalues of $A$, with
  associated eigenvectors $\vec{v}_1, \dots, \vec{v}_k$. Then,
  \begin{itemize}
  \item The eigenvectors are linearly independent.
  \item The sum of the eigenspaces $\Eig_A(\lambda_1) + \cdots +
    \Eig_A(\lambda_k)$ is a direct sum, i.e.\ 
    \[ \Eig_A(\lambda_1) + \cdots + \Eig_A(\lambda_k) \equiv
    \Eig_A(\lambda_1) \oplus \cdots \oplus \Eig_A(\lambda_k) \]
  \end{itemize}
\end{lemma}

\section{Lagrange's equations for oscillating systems}
\label{sec:lagrangian}
  Equations of motion in generalised coordinates, $q_i$, can be
  expressed in terms of the Lagrangian, ${\cal L}$, of the
  system. Each generalised coordinate has an associated generalised
  force, $\pdv{{\cal L}}{q_i}$, and a generalised momentum, $\dot{q} =
  \pdv{q_i}{t}$. This allows the equations of motion of the system to
  be expressed as
  \[ \dv{t} \qty( \pdv{{\cal L}}{\dot{q}_i} ) = \pdv{{\cal L}}{q_i} \]
  these can be treated as a matrix formulation. In an oscillating
  system the kinetic energy is quadratic with respect to the
  generalised momenta, and potential is quadratic with respect to the
  generalised coordinates. We thus have,
  \[ {\cal L} = \half m \dot{q}^{\rm T} T \dot{q} - \half k q^{\rm T}
  V q \] with $T$ and $V$ being matrices for kinetic and potential
  energies respectively.

  \begin{example}
    \emph{The characteristic frequencies of a system of springs.}\\
    \input{figures/springsystem.pgf} \\ Here $x$ and $y$ are generalised
    coordinates for the system. The kinetic energy is
    \[ T = \half m \qty( \dot{x}^2 + \dot{y}^2 ) \] and the potential
    is
    \[ V = \half k x^2 + \half k y^2 + \half k (x-y)^2 = \half k
    \qty(2 x^2 + 2y^2 - 2xy ) \] In matrix form this is
    \begin{align*}
      T &= \half m
      \begin{pmatrix}
        \dot{x} & \dot{y}
      \end{pmatrix}
      \begin{pmatrix}
        1 & 0 \\ 0 & 1
      \end{pmatrix}
      \begin{pmatrix}
        \dot{x} \\ \dot{y}
      \end{pmatrix} \\
      V &= \half k
      \begin{pmatrix}
        x & y
      \end{pmatrix}
      \begin{pmatrix}
        2 & -1 \\ -1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        x \\ y
      \end{pmatrix} \\
    \end{align*}
This can be solved by diagonalising the matrix 
\begin{align*}
  \begin{pmatrix}
    2 & -1 \\ -1 & 2
  \end{pmatrix}
\end{align*}
In order to do this we find the eigenvectors of $V$; the characteristic equation is 
\begin{align*}
  \qty|
  \begin{matrix}
    2 - \mu & -1 \\ -1 & 2 - \mu
  \end{matrix} | &= 0 \\
  (2 - \mu)(2-\mu)-1 &=0 \\
\mu^2 - 4 \mu + 3 &=0 \\
(\mu - 1)(\mu - 3) &= 0 \\
\mu &= 1 \quad \text{ or } \quad 3
\end{align*}
We have,
\begin{align*}
  \begin{pmatrix} x \\ y \end{pmatrix} =C \begin{pmatrix} x^{\prime}
    \\ y^{\prime}\end{pmatrix}
\end{align*}
a change of variables gives
\begin{align*}
  D = \begin{pmatrix} 1 & 0 \\ 0 & 3 \end{pmatrix}
\end{align*}
for $C^{-1}M C = D$, where
\[ V = \half k^{\prime} \qty( x^{\prime 2} + 3 y^{\prime 2} ) \]
$\dot{x}$ and $\dot{y}$ transform in the same way as $x$ and $y$, so
we have $T=I$, so
\[ C^{-1} T C = C^{-1} I C = C^{-1} C = I \] is unchanged, and so \[T
= \half m^{\prime} \qty( \dot{x}^{\prime 2} + \dot{y}^{\prime 2} ) \]
and 
\begin{equation*}
  {\cal L} = T - V = \half m^{\prime} \qty( \dot{x}^{\prime 2} + y^{\prime 2} ) - \half k^{\prime} \qty( x^{\prime 2} + 3 y^{\prime 2})
\end{equation*}

The two Lagrange equations are uncoupled (there are no $xy$ cross
terms), so they can be solved seperately,
\begin{align*}
  \dv{t} \qty( \pdv{{\cal L}}{\dot{x}^{\prime}} ) - \pdv{{\cal L}}{x^{\prime}} &= 0 \\
\dv{t} \qty( \half m \cdot 2 \dot{x}^{\prime} ) - \qty( - \half k \cdot 2 x^{\prime} ) &= 0 \\
m \ddot{x}^{\prime} + k x^{\prime} &= 0 \\
x^{\prime} &= A \sin(\omega_x t + \alpha) \\
\omega_x &= \sqrt{\frac{k}{m}}
\end{align*}
\begin{align*}
  \dv{t} \qty( \pdv{{\cal L}}{\dot{y}^{\prime}} ) - \pdv{{\cal L}}{y^{\prime}} &= 0 \\
\dv{t} \qty( \half m \cdot 2 \dot{y}^{\prime} ) - \qty( - \half k \cdot 6 y^{\prime} ) &= 0 \\
m \ddot{y}^{\prime} + k \cdot 3y{\prime} &= 0 \\
x^{\prime} &= B \sin(\omega_y t + \beta) \\
\omega_y &= \sqrt{\frac{3k}{m}}
\end{align*}
These normal modes then correspond to,\\
($\mu=1$), 
\begin{align*}
  \begin{pmatrix}    2 & -1 \\ -1 & 2  \end{pmatrix}
  \begin{pmatrix} x \\ y \end{pmatrix}
  &= \mu
  \begin{pmatrix} x \\ y \end{pmatrix} 
  =
  \begin{pmatrix} x \\ y \end{pmatrix} \\
  2x - y &= x \\
  y &= x
\end{align*}
($\mu=3$),
\begin{align*}
  \begin{pmatrix}2 & -1 \\ -1 & 2 \end{pmatrix}
  \begin{pmatrix}  x \\ y         \end{pmatrix}
  &= 3
  \begin{pmatrix} x \\ y \end{pmatrix} \\
  2x - y &= 3x \\
  y &= -x
\end{align*}
  \end{example}
